# Playing with LLMs

Using [Ollama](https://github.com/ollama/ollama) because it's free and can be run on my own hardware.

## todo

- [ ] compare model responses and runtimes
- [ ] inspect [temperature paramater](https://github.com/ollama/ollama-python/blob/89e8b74f1ea6b1af8a9df2e00e120a2a7e430311/ollama/_types.py#L170C3-L170C14)

## other links

- chat completion [message roles](https://platform.openai.com/docs/guides/chat-completions/message-roles)
- [chat vs generate](https://github.com/ollama/ollama/issues/2774)
